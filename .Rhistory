fit4 <- lm(y ~ xMat - 1)
summary(fit4)
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
knots <- seq(0, 8 * pi, length = 4);
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot))
xMat <- cbind(1, x, splineTerms)
yhat <- predict(lm(y ~ xMat - 1))
plot(x, y, frame = FALSE, pch = 21, bg = "lightblue", cex = 2)
lines(x, yhat, col = "red", lwd = 2)
fit4 <- lm(y ~ xMat - 1)
summary(fit4)
View(splineTerms)
knots <- seq(0, 8 * pi, length = 10);
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot))
xMat <- cbind(1, x, splineTerms)
yhat <- predict(lm(y ~ xMat - 1))
plot(x, y, frame = FALSE, pch = 21, bg = "lightblue", cex = 2)
lines(x, yhat, col = "red", lwd = 2)
fit4 <- lm(y ~ xMat - 1)
summary(fit4)
View(splineTerms)
knots <- seq(0, 8 * pi, length = 10);
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot))
xMat <- cbind(1, x, splineTerms[,2])
yhat <- predict(lm(y ~ xMat - 1))
plot(x, y, frame = FALSE, pch = 21, bg = "lightblue", cex = 2)
lines(x, yhat, col = "red", lwd = 2)
View(xMat)
fit4 <- lm(y ~ xMat - 1)
summary(fit4)
knots <- seq(0, 8 * pi, length = 10);
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot))
xMat <- cbind(x, splineTerms[,2])
yhat <- predict(lm(y ~ xMat - 1))
plot(x, y, frame = FALSE, pch = 21, bg = "lightblue", cex = 2)
lines(x, yhat, col = "red", lwd = 2)
fit4 <- lm(y ~ xMat)
summary(fit4)
knots <- seq(0, 8 * pi, length = 10);
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot))
xMat <- cbind(1, x, splineTerms[,2])
yhat <- predict(lm(y ~ xMat - 1))
plot(x, y, frame = FALSE, pch = 21, bg = "lightblue", cex = 2)
lines(x, yhat, col = "red", lwd = 2)
fit4 <- lm(y ~ xMat - 1)
summary(fit4)
knots <- seq(0, 8 * pi, length = 10);
splineTerms <- sapply(knots, function(knot) (x > knot) * (x - knot))
xMat <- cbind(1, x, splineTerms)
yhat <- predict(lm(y ~ xMat - 1))
plot(x, y, frame = FALSE, pch = 21, bg = "lightblue", cex = 2)
lines(x, yhat, col = "red", lwd = 2)
fit4 <- lm(y ~ xMat - 1)
summary(fit4)
View(xMat)
fit4 <- lm(y ~ xMat[,1:4] - 1)
summary(fit4)
View(xMat)
x <- -5:5
y <- c(5.12, 3.93, 2.67, 1.87, 0.52, 0.08, 0.93, 2.05, 2.54, 3.87, 4.97)
knot <- (x > 0) * x
fit4 <- lm(y ~ x + knot)
summary(fit4)
sum(coef(fit)[2:3])
sum(coef(fit4)[2:3])
sum(coef(fit4))
(coef(fit4))
sum(coef(fit4)[1:3])
sum(coef(fit4)[2:3])
sum(coef(fit4)[3:3])
sum(coef(fit4)[1:1])
sum(coef(fit4)[1:2])
a <- sum(coef(fit4))
class(x)
class(a)
a <- coef(fit4)
class(a)
a
t <- 5
count <- rpois(100, lamba = t)
k <- rep(c(1,0), 100)
library(Hmisc)
install.packages("Hmisc")
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
View(concrete)
View(concrete)
ggplot(data = concrete, mapping = aes(x = 1:nrow(concrete, y = concrete$CompressiveStrength))) + geom_point()
ggplot(data = concrete, mapping = aes(x = 1:nrow(concrete), y = concrete$CompressiveStrength))) + geom_point()
ggplot(data = concrete, mapping = aes(x = 1:nrow(concrete), y = concrete$CompressiveStrength)) + geom_point()
View(concrete)
head(concrete)
head(concrete)
ggplot(data = concrete, mapping = aes(x = 1:nrow(concrete), y = concrete$CompressiveStrength, fill = concrete$FlyAsh)) + geom_point()
?cut2
??cut2
library(Hmisc)
concrete$FlyAsh_f <- cut2(x = concrete$FlyAsh, cuts = 5, m = 150)
ggplot(data = concrete, mapping = aes(x = 1:nrow(concrete), y = concrete$CompressiveStrength, fill = concrete$FlyAsh_f)) + geom_point()
concrete$FlyAsh_f <- cut2(x = concrete$FlyAsh, cuts = 5)
ggplot(data = concrete, mapping = aes(x = 1:nrow(concrete), y = concrete$CompressiveStrength, fill = concrete$FlyAsh_f)) + geom_point()
ggplot(data = concrete, mapping = aes(x = 1:nrow(concrete), y = concrete$CompressiveStrength)) + geom_point(fill = concrete$FlyAsh_f)
ggplot(data = concrete, mapping = aes(x = 1:nrow(concrete), y = concrete$CompressiveStrength)) + geom_point(aes(fill = concrete$FlyAsh_f))
table(concrete$FlyAsh_f)
summary(concrete$FlyAsh)
concrete$FlyAsh_f <- cut2(x = concrete$FlyAsh, g = 5)
table(concrete$FlyAsh_f)
concrete$FlyAsh_f <- cut2(x = concrete$FlyAsh, g = 10)
table(concrete$FlyAsh_f)
concrete$FlyAsh_f <- cut2(x = concrete$FlyAsh, g = 6)
table(concrete$FlyAsh_f)
concrete$FlyAsh_f <- cut2(x = concrete$FlyAsh, m = 100)
table(concrete$FlyAsh_f)
ggplot(data = concrete, mapping = aes(x = 1:nrow(concrete), y = concrete$CompressiveStrength)) + geom_point(aes(fill = concrete$FlyAsh_f))
ggplot(data = concrete, mapping = aes(x = 1:nrow(concrete), y = concrete$CompressiveStrength)) + geom_point(aes(color = concrete$FlyAsh_f))
concrete$Age_f <- cut2(x = concrete$Age, m = 100)
ggplot(data = concrete, mapping = aes(x = 1:nrow(concrete), y = concrete$CompressiveStrength)) + geom_point(aes(color = concrete$Age_f))
ggplot(data = concrete, mapping = aes(x = 1:nrow(concrete), y = concrete$CompressiveStrength)) + geom_point(aes(color = concrete$FlyAsh_f))
ggplot(data = concrete, mapping = aes(x = concrete$FlyAsh, y = concrete$CompressiveStrength)) + geom_point()
ggplot(data = concrete, mapping = aes(x = 1:nrow(concrete), y = concrete$CompressiveStrength)) + geom_point(aes(color = concrete$FlyAsh_f))
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
training$FlyAsh_f <- cut2(x = training$FlyAsh, m = 100)
training$Age_f <- cut2(x = training$Age, m = 100)
ggplot(data = training, mapping = aes(x = 1:nrow(training), y = training$CompressiveStrength)) + geom_point(aes(color = training$FlyAsh_f))
ggplot(data = training, mapping = aes(x = 1:nrow(training), y = training$CompressiveStrength)) + geom_point(aes(color = training$Age_f))
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
ggplot(data = training, mapping = aes(training$Superplasticizer)) + geom_histogram()
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
adData = data.frame(predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain,]
testing = adData[-inTrain,]
library(dplyr)
training_IL <- select(.data = training, starts_with("IL"))
names(training_IL)
training_IL <- data.frame(diagnosis, training_IL)
names(training)
training_IL <- select(.data = training, starts_with("IL" | "diagnosis"))
training_IL <- select(.data = training, one_of(starts_with("IL" | "diagnosis")))
??select
?dplyr::select
?dplyr::select
training_diagnosis <- select(.data = training, matches("diagnosis"))
training_IL <- data.frame(training_diagnosis, training_IL)
names(training_IL)
preProc <- preProcess((training_IL[,-1],method="pca",pcaComp=2)
preProc <- preProcess(training_IL[,-1],method="pca",pcaComp=2)
trainPC <- predict(preProc,training_IL[,-1])
modelFit <- train(training$diagnosis ~ .,method="glm",data=trainPC)
confusionMatrix(training$diagnosis,predict(modelFit,trainPC))
summary(modelFit)
preProc <- preProcess(training_IL[,-1],method="pca")
trainPC <- predict(preProc,training_IL[,-1])
modelFit <- train(training$diagnosis ~ .,method="glm",data=trainPC)
confusionMatrix(training$diagnosis,predict(modelFit,trainPC))
summary(modelFit)
rm(list = ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
inTrain
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(dplyr)
training_IL <- select(.data = training, starts_with("IL")
training_diagnosis <- select(.data = training, matches("diagnosis"))
training_IL <- data.frame(training_diagnosis, training_IL)
names(training_IL)
training_IL <- select(.data = training, starts_with("IL"))
training_diagnosis <- select(.data = training, matches("diagnosis"))
training_IL <- data.frame(training_diagnosis, training_IL)
names(training_IL)
# build a predictive model with all predictors
modelFit1 <- train(training_IL$diagnosis ~ .,method="glm",data=training_IL)
summary(modelFit1)
preProc <- preProcess(training_IL[,-1],method="pca", thresh = 0.8)
trainPC <- predict(preProc,training_IL[,-1])
modelFit2 <- train(training_IL$diagnosis ~ .,method="glm",data=trainPC)
summary(modelFit2)
names(testing)
# apply each method to the test dataset
test_modelfit1 <- predict(modelFit1, testing[,-1])
confusionMatrix(testing$diagnosis, predict(test_modelfit1, testing))
test_modelfit1
summary(test_modelfit1)
confusionMatrix(testing$diagnosis, test_modelfit1)
testPC <- predict(preProc, testing[,-1])
confusionMatrix(testing$diagnosis, predict(modelFit2,testPC))
# apply each method to the test dataset and get accuracy measures
confusionMatrix(testing$diagnosis, predict(modelFit1, testing[,-1])) # accuracy = 0.6463
rm(list = ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain,]
testing = adData[-inTrain,]
# Find all the predictor variables in the training set that begin with IL. Perform principal components on these variables with the preProcess() function from the caret package. Calculate the number of principal components needed to capture 90% of the variance. How many are there?
# find all the predictor variables in the training set that begin with IL
library(dplyr)
training_IL <- select(.data = training, starts_with("IL")
training_diagnosis <- select(.data = training, matches("diagnosis"))
training_IL <- data.frame(training_diagnosis, training_IL)
names(training_IL)
# Perform principal components on these variables with the preProcess() function from the caret package. Calculate the number of principal components needed to capture 90% of the variance. How many are there?
preProc <- preProcess(training_IL[,-1],method="pca", thresh = 0.9)
trainPC <- predict(preProc,training_IL[,-1])
modelFit <- train(training_IL$diagnosis ~ .,method="glm",data=trainPC)
summary(modelFit) # there are 10 PCs retained
# 4. Load the Alzheimer's disease data using the commands:
rm(list = ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain,]
testing = adData[-inTrain,]
# Find all the predictor variables in the training set that begin with IL. Perform principal components on these variables with the preProcess() function from the caret package. Calculate the number of principal components needed to capture 90% of the variance. How many are there?
# find all the predictor variables in the training set that begin with IL
library(dplyr)
training_IL <- select(.data = training, starts_with("IL"))
training_diagnosis <- select(.data = training, matches("diagnosis"))
training_IL <- data.frame(training_diagnosis, training_IL)
names(training_IL)
# Perform principal components on these variables with the preProcess() function from the caret package. Calculate the number of principal components needed to capture 90% of the variance. How many are there?
preProc <- preProcess(training_IL[,-1],method="pca", thresh = 0.9)
trainPC <- predict(preProc,training_IL[,-1])
modelFit <- train(training_IL$diagnosis ~ .,method="glm",data=trainPC)
summary(modelFit) # there are 10 PCs retained
# Perform principal components on these variables with the preProcess() function from the caret package. Calculate the number of principal components needed to capture 90% of the variance. How many are there?
preProc <- preProcess(training_IL[,-1],method="pca", thresh = 0.8)
trainPC <- predict(preProc,training_IL[,-1])
modelFit <- train(training_IL$diagnosis ~ .,method="glm",data=trainPC)
summary(modelFit) # there are 9 PCs retained
?createDataPartition
# 4. Load the Alzheimer's disease data using the commands:
rm(list = ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain,]
testing = adData[-inTrain,]
# Find all the predictor variables in the training set that begin with IL. Perform principal components on these variables with the preProcess() function from the caret package. Calculate the number of principal components needed to capture 90% of the variance. How many are there?
# find all the predictor variables in the training set that begin with IL
library(dplyr)
training_IL <- select(.data = training, starts_with("IL"))
training_diagnosis <- select(.data = training, matches("diagnosis"))
training_IL <- data.frame(training_diagnosis, training_IL)
names(training_IL)
# Perform principal components on these variables with the preProcess() function from the caret package. Calculate the number of principal components needed to capture 80% of the variance. How many are there?
preProc <- preProcess(training_IL[,-1],method="pca", thresh = 0.9)
trainPC <- predict(preProc,training_IL[,-1])
modelFit <- train(training_IL$diagnosis ~ .,method="glm",data=trainPC)
summary(modelFit) # there are 7 PCs retained
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
# subset the data based on the Case variable
inTrain <- createDataPartition(y=segmentationOriginal$Case,
p=0.5, list=FALSE)
trainSegOri <- segmentationOriginal[inTrain,]
testSegOri <- segmentationOriginal[-inTrain,]
modFit <- train(Case ~ .,method="rpart",data=training)
print(modFit$finalModel)
modFit <- train(Case ~ .,method="rpart",data=trainSegOri)
print(modFit$finalModel)
library(rattle)
install.packages("rattle")
library(rattle)
fancyRpartPlot(modFit$finalModel)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
# 1. Subset the data to a training set and testing set based on the Case variable in the data set.
inTrain <- createDataPartition(y=segmentationOriginal$Case,
p=0.5, list=FALSE)
trainSegOri <- segmentationOriginal[inTrain,]
testSegOri <- segmentationOriginal[-inTrain,]
# 2. Set the seed to 125 and fit a CART model with the rpart method using all predictor variables and default caret settings.
set.seed(125)
modFit <- train(Case ~ .,method="rpart",data=trainSegOri)
print(modFit$finalModel)
library(rattle)
?fancyRpartPlot
fancyRpartPlot(modFit$finalModel)
fancyRpartPlot(modFit)
install.packages("rpart.plot")
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
table(segmentationOriginal$Case)
?predict
predict(object = modFit, newdata = test)
predict(object = modFit, newdata = testSegOri)
pred1 <- predict(object = modFit, newdata = testSegOri)
pred1 <- predict.train(object = modFit, newdata = testSegOri)
str(segmentationOriginal)
trainSegOri <- segmentationOriginal[segmentationOriginal$Case == "Train", ]
testSegOri <- segmentationOriginal[segmentationOriginal$Case == "Test",]
set.seed(125)
modFit <- train(Case ~ .,method="rpart",data=trainSegOri)
rm(pred1)
modFit <- train(Class ~ .,method="rpart",data=trainSegOri)
print(modFit$finalModel)
library(rattle)
library(rpart.plot)
fancyRpartPlot(modFit$finalModel)
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
set.seed(125)
modFit <- train(Area ~ .,method="rpart",data=oilve)
modFit <- train(Area ~ .,method="rpart",data=olive)
predict(object = modFit$finalModel, newdata = as.data.frame(t(colMeans(olive))))
print(modFit$finalModel)
rm(list = ls())
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
names(SAheart)
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data = trainSA, method = "glm", family = "binomial")
(getModelInfo())
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data = trainSA, method = "glm", family = binomial)
missClass = function(values,prediction) {
sum(((prediction > 0.5)*1) != values)/length(values)
}
?predict
?train.predict
?predict.train
missClass(trainSA,modFit)
missClass(trainSA,predict(modFit,trainSA)
)
missClass(trainSA,predict(modFit,newdata = trainSA, type = "raw"))
missClass(trainSA,predict(modFit,newdata = testSA, type = "raw"))
set.seed(13234)
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data = trainSA, method = "glm", family = binomial)
# Calculate the misclassification rate for your model using this function and a prediction on the "response" scale:
missClass = function(values,prediction) {
sum(((prediction > 0.5)*1) != values)/length(values)
}
missClass(trainSA,predict(modFit,newdata = trainSA, type = "raw"))
missClass(trainSA,predict(modFit,newdata = testSA, type = "raw"))
missClass(trainSA,predict(modFit$finalModel,newdata = trainSA, type = "raw"))
missClass(trainSA,predict(modFit$finalModel,newdata = testSA, type = "raw"))
missClass(trainSA,predict(modFit$finalModel,newdata = trainSA, type = "response"))
missClass(trainSA,predict(modFit$finalModel,newdata = testSA, type = "response"))
modelfit2 <- lm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data = trainSA, method = "glm", family = binomial)
set.seed(13234)
modelfit2 <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data = trainSA, family = binomial)
pred1 <- predict(modelfit2,newdata=trainSA,type = "response")
missClass(trainSA$chd, pred1)
pred2 <- predict(modelfit2,newdata=testSA,type = "response")
missClass(testSA$chd, pred2) # 0.2727273
missClass(trainSA$chd,predict(modFit$finalModel,newdata = trainSA, type = "response"))
missClass(testSA$chd,predict(modFit$finalModel,newdata = testSA, type = "response"))
library(pacman)
p_unload(rattle)
detach("package:pgmm", unload=TRUE)
detach("package:rpart.plot", unload=TRUE)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
class(vowel.train)
class(vowel.test)
names(vowel.test)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
names(vowel.train)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
names(vowel.test)
names(vowel.train)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
# Then set the seed to 33833.
set.seed(33833)
modFit <- train(y ~ ., data=vowel.train, method="rf", prox=TRUE)
library(caret)
modFit <- train(y ~ ., data=vowel.train, method="rf", prox=TRUE)
modFit <- train(y ~ ., data=vowel.train, method="rf", prox=TRUE)
modFit
?varImp
varImp(modFit)
# Then set the seed to 33833.
set.seed(33833)
# Fit a random forest predictor relating the factor variable y to the remaining variables.
library(caret)
modFit <- train(y ~ ., data=vowel.train, method="rf", prox=TRUE)
modFit
varImp(modFit) # gives the vars in order of importance
varImpPlot(modFit)
rf.vowel = randomForest(y ~ ., data = vowel.train),
rf.vowel = randomForest(y ~ ., data = vowel.train)
varImpPlot(rf.vowel)
varImp(rf.vowel)
order(varImp(rf.vowel))
- order(varImp(rf.vowel))
order(varImp(rf.vowel), desc)
?order
order(varImp(rf.vowel), decreasing = T)
# Q1 Load the cell segmentation data from the AppliedPredictiveModeling package using the commands:
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
# 1. Subset the data to a training set and testing set based on the Case variable in the data set.
table(segmentationOriginal$Case)
# how I would do it if there was no Case variable already splitting up the dataset
# inTrain <- createDataPartition(y=segmentationOriginal$Case,
#                                p=0.5, list=FALSE)
# trainSegOri <- segmentationOriginal[inTrain,]
# testSegOri <- segmentationOriginal[-inTrain,]
trainSegOri <- segmentationOriginal[segmentationOriginal$Case == "Train", ]
testSegOri <- segmentationOriginal[segmentationOriginal$Case == "Test",]
set.seed(125)
modFit <- train(Class ~ .,method="rpart",data=trainSegOri)
print(modFit$finalModel)
library(rpart.plot)
rpart.plot(modFit)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
print(modFit$finalModel) # this gives the answers to the questions
devtools::install_github('ropensci/plotly')
library(plotly)
install.packages(c("boot", "devtools", "effects", "gridExtra", "gtable", "Hmisc", "knitr", "lme4", "maps", "memisc", "memoise", "mgcv", "multcomp", "mvtnorm", "nlme", "nnet", "openssl", "pbkrtest", "quantreg", "RcppEigen", "rmarkdown", "scales", "sfsmisc", "statmod", "texreg", "TH.data", "tidyr", "xtable"))
getwd()
library(plotly)
?detach
detach(package:plotly, unload = true)
detach(package:plotly, unload = T)
detach("package:plotly", unload = T)
detach(package:plotly, unload = T)
detach("package::plotly", unload = T)
detach("package:plotly", unload = T)
?search
search()
Sys.setenv("plotly_username"="mparam83")
Sys.setenv("plotly_api_key"="4lzn2zidhj")
library(plotly)
p <- plot_ly(midwest, x = percollege, color = state, type = "box")
plotly_POST(p, filename = "r-docs/midwest-boxplots", world_readable=FALSE)
p <- plot_ly(midwest, x = percollege, color = state, type = "box")
p
plotly_POST(p, filename = "r-docs/midwest-boxplots", world_readable=FALSE)
?plotly_POST
plotly_POST(p, filename = "midwest-boxplots-practice", sharing = "private")
setwd("~/GitHub/practice_slidify")
library(slidify)
library(slidifyLibraries)
library(knitr)
runDeck()
browseURL("index.html")
runDeck()
runDeck()
runDeck()
runDeck()
runDeck()
publish_github(repo = "practice_slidify", username = meenaparam)
publish_github(repo = "practice_slidify", username = "meenaparam")
